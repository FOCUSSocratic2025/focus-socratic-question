# -*- coding: utf-8 -*-
"""llama2-focus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JR16VKRLg6OYw3vCZgVG7uqLha1Krcz3
"""

pip install huggingface_hub
huggingface-cli login
# 0) Install dependencies
pip install --quiet \
    accelerate \
    peft \
    bitsandbytes \
    transformers \
    datasets \
    pandas \
    huggingface_hub

# 1) Imports
import torch
import pandas as pd
from huggingface_hub import hf_hub_download
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    Trainer,
)
from peft import LoraConfig, get_peft_model, TaskType

# 2) Constants
BASE_MODEL = "meta-llama/Llama-2-7b-hf"
OUTPUT_DIR = "./lora_llama2"
REPO_ID    = "Pothong/fsq-lora-v2"
FILENAME   = "fsq_with_span_mistral_llama.csv"

# 3) Download CSV from HF Hub
print(f"Downloading {FILENAME} from {REPO_ID}‚Ä¶")
csv_path = hf_hub_download(
    repo_id=REPO_ID,
    repo_type="dataset",
    filename=FILENAME,
    revision="main"
)
print("Local path:", csv_path)

# 4) Load with pandas
df = pd.read_csv(csv_path)
print(f"Loaded {len(df)} rows; columns: {df.columns.tolist()}")

# 5) Convert to ü§ó Dataset
hf_ds = Dataset.from_pandas(df.reset_index(drop=True))
print("Dataset ready:", hf_ds)

# (optional) split into train/test
hf_ds = hf_ds.train_test_split(test_size=0.1)

# 6) Load model & tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
# ‚Äî add a pad token so collator works smoothly ‚Äî
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"

bnb_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto",
)
# ‚Äî disable kv‚Äêcache for LoRA training ‚Äî
model.config.use_cache = False

# 7) Attach LoRA (only to Q/V proj matrices)
lora_cfg = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"],
)
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

# 8) Preprocessing
def preprocess_fn(examples):
    prompts = [
        f"{ins}\n\n{inp}\n\nResponse:"
        for ins, inp in zip(examples["instruction"], examples["input"])
    ]
    tokenized_inputs = tokenizer(
        prompts,
        max_length=512,
        truncation=True,
        padding="longest",
    )
    labels = tokenizer(
        examples["output"],
        max_length=128,
        truncation=True,
        padding="longest",
    )["input_ids"]
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

hf_ds = hf_ds.map(
    preprocess_fn,
    batched=True,
    batch_size=16,
    remove_columns=hf_ds["train"].column_names,
)

# 9) Training setup
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    logging_steps=20,
    fp16=True,
    save_strategy="epoch",
    save_total_limit=2,
    logging_dir=f"{OUTPUT_DIR}/logs",
)

# ‚Äî use a causal‚ÄêLM collator, not Seq2Seq ‚Äî
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=hf_ds["train"],
    eval_dataset=hf_ds.get("test"),
    data_collator=data_collator,
    tokenizer=tokenizer,
)

# 10) Train & save
trainer.train()
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)